---
import ChatMessage from "../../../components/chat/ChatMessage.astro"
import OpenAI from "openai"
import { isUserWithinRateLimit, isHtmxRequest } from "../../../utils";
import { baseSystemPrompt, instructionsForSuspiciousInputs, allowedUserInputs, defaultInput } from "../../../chatConfig";

export const prerender = false

if (!isHtmxRequest(Astro.request)) {
    return Astro.redirect('/')
}
if (!isUserWithinRateLimit(Astro.cookies)) {
    return Astro.redirect('/chat/rate_limit_exceeded')
}

const openai = new OpenAI({ apiKey: import.meta.env.OPENAI_API_KEY })

let userInputFromQuery = Astro.url.searchParams.get('user_input')
let isUserInputAllowed = allowedUserInputs.includes(userInputFromQuery)
let finalUserInput = isUserInputAllowed ? userInputFromQuery : defaultInput
let finalSystemPrompt = isUserInputAllowed ? baseSystemPrompt : baseSystemPrompt + instructionsForSuspiciousInputs

let chatHistory = Astro.cookies.has('chat-history') ? JSON.parse(Astro.cookies.get('chat-history').value) : [];
chatHistory = chatHistory.filter(msg => msg.role === "assistant" || msg.role === "system" || (msg.role === "user" && allowedUserInputs.includes(msg.content))).slice(-6)
chatHistory.push({ role: "user", content: finalUserInput })

const gptChatResponse = await openai.chat.completions.create({
    messages: [
        { role: "system", content: finalSystemPrompt },
        { role: "assistant", content: "Hey chief. I'm here to help you become a 10x engineer." },
        { role: "user", content: "I'm not ready to become a 10x engineer..." },
        ...chatHistory
    ],
    model: "ft:gpt-3.5-turbo-0613:personal::7teyZSgg",
    max_tokens: 125,
})

let gptResponse = gptChatResponse.choices[0].message.content
chatHistory.push({ role: "assistant", content: gptResponse })
Astro.cookies.set("chat-history", JSON.stringify(chatHistory))
---
<ChatMessage sender='gpt' text={gptResponse} />
